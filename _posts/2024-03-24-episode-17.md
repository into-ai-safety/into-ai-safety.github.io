---
title: "INTERVIEW: StakeOut.AI w/ Dr. Peter Park (3)"
date: 2024-03-24T00:00:00-06:00
categories:
  - Episode
  - Interview
tags:
  - StakeOut.AI Feb 2024
  - Starter
  - AISC
  - Ep.17
---

As always, the best things come in 3s: dimensions, musketeers, pyramids, and... 3 installments of my interview with Dr. Peter Park, a post-doctoral fellow at MIT working with Dr. Max Tegmark on AI existential safety.

As you may have ascertained from the previous two segments of the interview, Dr. Park cofounded <a href="https://www.stakeout.ai" target="_blank" rel="noreferrer noopener">StakeOut.AI</a> along with Harry Luk and one other cofounder whose name has been removed due to requirements of her current position. The non-profit had a simple but important mission: make the adoption of AI technology go well, for humanity, but unfortunately, StakeOut.AI had to dissolve in early March of 2024 because no granter would fund them. Although it certainly is dissapointing that the organization is no longer functioning, all three cofounders continue to contribute positively towards improving our world in their current roles.

<audio controls>
<source src="https://into-ai-safety.github.io/assets\audio\into-ai-safety_ep.17.mp3" type="audio/mp3">
</audio>

If you would like to investigate further into Dr. Park's work, view his <a href="https://scholar.harvard.edu/pspark" target="_blank" rel="noreferrer noopener">website</a>, <a href="https://scholar.google.com/citations?user=5lMAPEoAAAAJ&hl=en" target="_blank" rel="noreferrer noopener">Google Scholar</a>, or follow him on <a href="https://twitter.com/dr_park_phd" target="_blank" rel="noreferrer noopener">Twitter</a>!


<!-- ### Chapters

00:00:54 - Intro<br>
00:02:34 - <br>
00:06:28 - <br>
00:08:15 - <br>
00:20:41 - <br>
00:34:28 - <br>
00:37:57 - <br>
00:40:02 - <br>
00:47:40 - <br>
00:50:00 - <br>
00:53:07 - <br>
00:54:43 - <br>
01:05:57 -  -->

### Links

Links to all articles/papers which are mentioned throughout the episode can be found below, in order of their appearance.
- <a href="https://www.stakeout.ai" target="_blank" rel="noreferrer noopener">StakeOut.AI</a>
- <a href="https://pauseai.info" target="_blank" rel="noreferrer noopener">Pause AI</a>
- <a href="https://futureoflife.org/wp-content/uploads/2023/11/FLI_Governance_Scorecard_and_Framework.pdf" target="_blank" rel="noreferrer noopener">AI Governance Scorecard</a> (go to Pg. 3)
- <a href="https://civitai.com" target="_blank" rel="noreferrer noopener">CIVITAI</a>
  - <a href="https://www.404media.co/a16z-funded-ai-platform-generated-images-that-could-be-categorized-as-child-pornography-leaked-documents-show/" target="_blank" rel="noreferrer noopener">Article on CIVITAI and CSAM</a>
- <a href="https://www.judiciary.senate.gov/protecting-children-online" target="_blank" rel="noreferrer noopener">Senate Hearing: Protecting Children Online</a>
  - <a href="https://www.pbs.org/newshour/politics/watch-live-ceos-of-meta-tiktok-x-and-other-social-media-companies-testify-in-senate-hearing" target="_blank" rel="noreferrer noopener">PBS Newshour Coverage</a>
- <a href="https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html" target="_blank" rel="noreferrer noopener">The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work</a>
- Open Source/Weights/Release/Interpretation
  - <a href="https://opensource.org" target="_blank" rel="noreferrer noopener">Open Source Initiative</a>
    - <a href="https://opensource.org/history" target="_blank" rel="noreferrer noopener">History of the OSI</a>
    - <a href="https://opensource.org/blog/metas-llama-2-license-is-not-open-source" target="_blank" rel="noreferrer noopener">Meta’s LLaMa 2 license is not Open Source</a>
  - <a href="https://opensourceconnections.com/blog/2023/07/19/is-llama-2-open-source-no-and-perhaps-we-need-a-new-definition-of-open/" target="_blank" rel="noreferrer noopener">Is Llama 2 open source? No – and perhaps we need a new definition of open…</a>
  - <a href="https://www.apache.org/licenses/LICENSE-2.0" target="_blank" rel="noreferrer noopener">Apache License, Version 2.0</a>
  - <a href="https://www.3blue1brown.com/topics/neural-networks" target="_blank" rel="noreferrer noopener">3Blue1Brown: Neural Networks</a>
  - <a href="https://dl.acm.org/doi/10.1145/3571884.3604316" target="_blank" rel="noreferrer noopener">Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators</a>
    - The online <a href="https://opening-up-chatgpt.github.io" target="_blank" rel="noreferrer noopener">table</a>
- <a href="https://www.signal.org" target="_blank" rel="noreferrer noopener">Signal</a>
- <a href="https://huggingface.co/bigscience/bloomz" target="_blank" rel="noreferrer noopener">Bloomz</a> model on HuggingFace
- <a href="https://mistral.ai" target="_blank" rel="noreferrer noopener">Mistral</a> website
- NASA Tragedies
  - <a href="https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster" target="_blank" rel="noreferrer noopener">Challenger disaster</a> on Wikipedia
  - <a href="https://en.wikipedia.org/wiki/Space_Shuttle_Columbia_disaster" target="_blank" rel="noreferrer noopener">Columbia disaster</a> on Wikipedia
- AIxBio Risk
  - <a href="https://www.nature.com/articles/s42256-022-00465-9" target="_blank" rel="noreferrer noopener">Dual use of artificial-intelligence-powered drug discovery</a>
  - <a href="https://arxiv.org/abs/2306.03809" target="_blank" rel="noreferrer noopener">Can large language models democratize access to dual-use biotechnology?</a>
  - <a href="https://www.governance.ai/research-paper/open-sourcing-highly-capable-foundation-models" target="_blank" rel="noreferrer noopener">Open-Sourcing Highly Capable Foundation Models</a> _(sadly, I can't rename the article...)_
  - <a href="https://1a3orn.com/sub/essays-propaganda-or-science.html" target="_blank" rel="noreferrer noopener">Propaganda or Science: Open Source AI and Bioterrorism Risk</a>
  - <a href="https://ineffectivealtruismblog.com/2024/03/09/exaggerating-the-risks-part-14-biorisk-from-llms/" target="_blank" rel="noreferrer noopener">Exaggerating the risks (Part 15: Biorisk from LLMs)</a>
  - <a href="https://arxiv.org/abs/2310.18233" target="_blank" rel="noreferrer noopener">Will releasing the weights of future large language models grant widespread access to pandemic agents?</a>
  - <a href="https://crfm.stanford.edu/open-fms/" target="_blank" rel="noreferrer noopener">On the Societal Impact of Open Foundation Models</a>
    - <a href="https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf" target="_blank" rel="noreferrer noopener">Policy brief</a>
- <a href="https://www.apartresearch.com" target="_blank" rel="noreferrer noopener">Apart Research</a>
- <a href="https://www.science.org" target="_blank" rel="noreferrer noopener">Science</a>
- Cicero
  - <a href="https://www.science.org/doi/10.1126/science.ade9097" target="_blank" rel="noreferrer noopener">Human-level play in the game of Diplomacy by combining language models with strategic reasoning</a>
  - <a href="https://ai.meta.com/research/cicero/" target="_blank" rel="noreferrer noopener">Cicero</a> webpage
  - <a href="https://arxiv.org/abs/2308.14752" target="_blank" rel="noreferrer noopener">AI Deception: A Survey of Examples, Risks, and Potential Solutions</a>
- <a href="https://aisafety.camp" target="_blank" rel="noreferrer noopener">AI Safety Camp</a>
- <a href="https://www.patreon.com/IntoAISafety" target="_blank" rel="noreferrer noopener">Into AI Safety Patreon</a>

<!-- end of the list -->
